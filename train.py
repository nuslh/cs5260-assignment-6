# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QpXeYTPEr44R47lTv0gjZZquhCsiFFJe

# Install Colossalai
"""

!pip install colossalai

"""# Import Libraires"""

import os
from pathlib import Path
import math
import colossalai
import torch
import torch.nn as nn
import torch.nn.functional as F
from colossalai.core import global_context as gpc
from colossalai.logging import get_dist_logger
from colossalai.nn import CosineAnnealingLR
from colossalai.nn.metric import Accuracy
from colossalai.trainer import Trainer, hooks
from colossalai.utils import MultiTimer, get_dataloader
from torchvision import transforms
from torchvision.datasets import MNIST
from tqdm import tqdm

import pandas as pd
import time
import multiprocessing as mp
#import torch.multiprocessing as mp
#import torch.distributed as dist
import nest_asyncio
nest_asyncio.apply()

"""# Define LeNet5"""

class LeNet5(nn.Module):

    def __init__(self, n_classes):
        super(LeNet5, self).__init__()

        self.feature_extractor = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1),
            nn.Tanh(),
            nn.AvgPool2d(kernel_size=2),
            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),
            nn.Tanh(),
            nn.AvgPool2d(kernel_size=2),
            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1),
            nn.Tanh()
        )

        self.classifier = nn.Sequential(
            nn.Linear(in_features=120, out_features=84),
            nn.Tanh(),
            nn.Linear(in_features=84, out_features=n_classes),
        )

    def forward(self, x):
        x = self.feature_extractor(x)
        x = torch.flatten(x, 1)
        logits = self.classifier(x)
        probs = F.softmax(logits, dim=1)
        return logits

"""# Optimizer_Switcher"""

def optimizer_switcher(optimizer,model,learning_rate):
    weight_decay=5e-4
    momentum=0.9
    beta1=0.9
    beta2=0.999
    if optimizer == 'sgd':
        return torch.optim.SGD(model.parameters(), learning_rate, momentum=momentum,
                         weight_decay=weight_decay)
    elif optimizer == 'adagrad':
        return torch.optim.Adagrad(model.parameters(), learning_rate, weight_decay=weight_decay)
    elif optimizer == 'adam':
        
        return torch.optim.Adam(model.parameters(), learning_rate, betas=(beta1, beta2),
                          weight_decay=weight_decay)
    elif optimizer == 'amsgrad':
        return torch.optim.Adam(model.parameters(), learning_rate, betas=(beta1, beta2),
                          weight_decay=weight_decay, amsgrad=True)

"""# Learning_rate_scheduler_switcher"""

def learning_rate_scheduler_switcher(schedule_type, optimizer,t_dataloader,gpc):
    def lrs_exponential(batch):
        low = math.log2(1e-5)
        high = math.log2(10)
        return 2**(low+(high-low)*batch/len(t_dataloader)/gpc.config.NUM_EPOCHS)

    if schedule_type == 'step':
        milestones=[30,80]
        gamma_default=0.1
        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma_default)
    


    elif schedule_type == 'poly':
        total_step = (len(t_dataloader) / gpc.config.BATCH_SIZE+ 1) * gpc.config.NUM_EPOCHS
        
        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda x: (1-x/total_step) ** 2)

    
    elif schedule_type == 'constant':
        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda x: 1)
    
    elif schedule_type == 'cosine':
        

        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)
    
    elif schedule_type =='exponential':

        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lrs_exponential)
    return scheduler

"""# Single Thread training"""

def thread_train(model,config,optim,learning_rate_scheduler,initial_lr,logger):
    

    optimizer=optimizer_switcher(optim,model,initial_lr)
    criterion = torch.nn.CrossEntropyLoss()
    log_directory='./'+optim+' and '+learning_rate_scheduler

    train_dataset = MNIST(
      root=Path('./tmp/'),
      download=True,
      transform = transforms.Compose([transforms.Resize((32, 32)),
                              transforms.ToTensor()])
    )

    test_dataset = MNIST(
      root=Path('./tmp/'),
      train=False,
      transform = transforms.Compose([transforms.Resize((32, 32)),
                              transforms.ToTensor()])
    )

    train_dataloader = get_dataloader(dataset=train_dataset,
                                  shuffle=True,
                                  batch_size=gpc.config.BATCH_SIZE,
                                  num_workers=1,
                                  pin_memory=True,
                                  )

    test_dataloader = get_dataloader(dataset=test_dataset,
                                  add_sampler=False,
                                  batch_size=gpc.config.BATCH_SIZE,
                                  num_workers=1,
                                  pin_memory=True,
                                  )


    # lr_scheduler
    lr_scheduler = learning_rate_scheduler_switcher(learning_rate_scheduler,optimizer,train_dataloader,gpc)

    engine, train_dataloader, test_dataloader, _ = colossalai.initialize(model,
                                                                      optimizer,
                                                                      criterion,
                                                                      train_dataloader,
                                                                      test_dataloader,
                                                                      )
    # build a timer to measure time
    timer = MultiTimer()

    # create a trainer object
    trainer = Trainer(
       engine=engine,
        timer=timer,
        logger=logger
    )

    # define the hooks to attach to the trainer
    hook_list = [
        hooks.LossHook(),
        hooks.LRSchedulerHook(lr_scheduler=lr_scheduler, by_epoch=False),
        hooks.AccuracyHook(accuracy_func=Accuracy()),
        hooks.LogMetricByEpochHook(logger),
        hooks.LogMemoryByEpochHook(logger),
        hooks.LogTimingByEpochHook(timer, logger),

        # you can uncomment these lines if you wish to use them
        hooks.TensorboardHook(log_dir=log_directory, ranks=[0]),
        hooks.SaveCheckpointHook(checkpoint_dir='./ckpt')
    ]

    # start training
    trainer.fit(
        train_dataloader=train_dataloader,
        epochs=gpc.config.NUM_EPOCHS,
        test_dataloader=test_dataloader,
        test_interval=1,
        hooks=hook_list,
        display_progress=True
    )
    torch.cuda.empty_cache()
   # print('done')
    #return 0

"""# Main Function"""

if __name__=="__main__":

  config = {'BATCH_SIZE':128,'NUM_EPOCHS':30}
  optimizer_list=['sgd','adam']
  colossalai.launch(config=config,rank=0,world_size=1,host='127.0.0.1',port=1234)
  port_number=1234
  init_lr=0.1
  logger = get_dist_logger()
  learning_rate_scheduler_list=['step','constant','exponential']
  #num_processes=len(optimizer_list)*len(learning_rate_scheduler_list)
  model = LeNet5(n_classes=10)
  model.share_memory()
  #processes=[]
  #rank_num=0
  #q = ctx.Queue()
  for optimizer in optimizer_list:

    for learning_rate_scheduler in learning_rate_scheduler_list:
      thread_train(model,config,optimizer,learning_rate_scheduler,init_lr,logger)

"""# Log Viewer"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
optimizer_list=['sgd','adam']
learning_rate_scheduler_list=['step','constant','exponential']
log_list=[]
for optimizer in optimizer_list:
    for learning_rate_scheduler in learning_rate_scheduler_list:
      log_directory=optimizer+' and '+learning_rate_scheduler
      print('log file:',log_directory)
#       %cd /content/$log_directory
#       %tensorboard --logdir .

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard